# Calculate residuals and predicted values
residuals <- model$residuals
data("mtcars")
# Fit the model
model_a <- lm(mpg ~ hp, data=mtcars)
# Plot the data and the regression line
plot(mtcars$hp, mtcars$mpg, main="Regression of mpg on hp",
xlab="Horsepower (hp)", ylab="Miles per Gallon (mpg)", pch=16)
# Adds the regression line to the plot
abline(model_a, col="red")
summary(model)$r.squared
data("mtcars")
# Fit the model
model_a <- lm(mpg ~ hp, data=mtcars)
# Plot the data and the regression line
plot(mtcars$hp, mtcars$mpg, main="Regression of mpg on hp",
xlab="Horsepower (hp)", ylab="Miles per Gallon (mpg)", pch=16)
# Adds the regression line to the plot
abline(model_a, col="red")
summary(model_a)$r.squared
#The R2 of this regression is 0.602
# Calculate residuals and predicted values
residuals <- model$residuals
data("mtcars")
# Fit the model
model_a <- lm(mpg ~ hp, data=mtcars)
# Plot the data and the regression line
plot(mtcars$hp, mtcars$mpg, main="Regression of mpg on hp",
xlab="Horsepower (hp)", ylab="Miles per Gallon (mpg)", pch=16)
# Adds the regression line to the plot
abline(model_a, col="red")
summary(model_a)$r.squared
#The R2 of this regression is 0.602
# Calculate residuals and predicted values
residuals <- model_a$residuals
fitted_values <- model_a$fitted.values
# Plot residuals vs. fitted values
plot(fitted_values, residuals, main="Residuals vs. Fitted Values",
xlab="Fitted Values", ylab="Residuals", pch=16)
# Adds a horizontal line at zero
abline(h=0, col="red")
#Based On the plot below shows, the data appear to be linear. Through the common method of plotting the residuals versus the predicted values (or the independent variable). It seen as the  residuals to be randomly scattered around zero without any clear pattern, thus the relationship is approximately linear.
mtcars$log_hp <- log(mtcars$hp)
model_log <- lm(mpg ~ log_hp, data=mtcars)
# Check the new R²
summary(model_log)$r.squared
#Use
model_poly <- lm(mpg ~ hp + I(hp^2), data=mtcars)
# Check the new R²
summary(model_poly)$r.squared
#
# Using caret for cross-validation
library(ggplot2)
library(caret)
library(lattice)
# Define training control
train_control <- trainControl(method="cv", number=10)  # 10-fold CV
# Training model with train function (it will try different values of k)
model <- train(Species ~ ., data=train_set, method="knn", trControl=train_control, tuneGrid=data.frame(k=c(3,5,7)))
data("mtcars")
# Fit the model
model_a <- lm(mpg ~ hp, data=mtcars)
# Plot the data and the regression line
plot(mtcars$hp, mtcars$mpg, main="Regression of mpg on hp",
xlab="Horsepower (hp)", ylab="Miles per Gallon (mpg)", pch=16)
# Adds the regression line to the plot
abline(model_a, col="red")
summary(model_a)$r.squared
#The R2 of this regression is 0.602
# Calculate residuals and predicted values
residuals <- model_a$residuals
fitted_values <- model_a$fitted.values
# Plot residuals vs. fitted values
plot(fitted_values, residuals, main="Residuals vs. Fitted Values",
xlab="Fitted Values", ylab="Residuals", pch=16)
# Adds a horizontal line at zero
abline(h=0, col="red")
#Based On the plot below shows, the data appear to be linear. Through the common method of plotting the residuals versus the predicted values (or the independent variable). It seen as the  residuals to be randomly scattered around zero without any clear pattern, thus the relationship is approximately linear.
mtcars$log_hp <- log(mtcars$hp)
model_log <- lm(mpg ~ log_hp, data=mtcars)
# Check the new R²
summary(model_log)$r.squared
#Use
model_poly <- lm(mpg ~ hp + I(hp^2), data=mtcars)
# Check the new R²
summary(model_poly)$r.squared
# Based on the increase in R^2 value when using the log transformation of horsepower, it suggests that the log transformation of hp indeed improved the model fit to the data. The R² value increased from 72.04% to 75.61%, which indicates that the log-transformed model explains more of the variability in mpg than the original model.
# In conclusion, while the log transformation improved the R², it's essential to consider other diagnostic plots and tests to ensure that the assumptions of linear regression are met.
# Calculate residuals and predicted values
residuals <- model_a$residuals
fitted_values <- model_a$fitted.values
# Plot residuals vs. fitted values
plot(fitted_values, residuals, main="Residuals vs. Fitted Values",
xlab="Fitted Values", ylab="Residuals", pch=16)
# Adds a horizontal line at zero
abline(h=0, col="red")
#Based On the plot below shows, the data appear to be linear. Through the common method of plotting the residuals versus the predicted values (or the independent variable). It seen as the  residuals to be randomly scattered around zero without any clear pattern, thus the relationship is approximately linear.
mtcars$log_hp <- log(mtcars$hp)
model_log <- lm(mpg ~ log_hp, data=mtcars)
# Check the new R²
summary(model_log)$r.squared
#Use
model_poly <- lm(mpg ~ hp + I(hp^2), data=mtcars)
# Check the new R²
summary(model_poly)$r.squared
# Based on the increase in R^2 value when using the log transformation of horsepower, it suggests that the log transformation of hp indeed improved the model fit to the data. The R² value increased from 72.04% to 75.61%, which indicates that the log-transformed model explains more of the variability in mpg than the original model.
# In conclusion, while the log transformation improved the R², it's essential to consider other diagnostic plots and tests to ensure that the assumptions of linear regression are met.
# Create a sequence of model complexities (e.g., number of features, polynomial degree, etc.)
complexities <- seq(1, 100, by=1)
# Generate hypothetical training and test error data based on the general trend
train_error <- 1 / (1 + exp(0.1 * (complexities - 50)))
test_error <- train_error + (complexities - 50)^2 / 2500
# Plot the errors
plot(complexities, train_error, type="l", col="blue", ylim=c(0,1),
ylab="Error", xlab="Model Complexity", main="Training vs. Test Error")
lines(complexities, test_error, col="red")
legend("topright", legend=c("Training Error", "Test Error"), fill=c("blue", "red"))
# The blue line represents training error.
# The red line represents test error.
# The plot shows that as model complexity increases, training error generally decreases. Meanwhile, test error decreases at first but then starts to increase, indicating overfitting.
# Keep in mind that this is a hypothetical representation to demonstrate the concept. In real-world scenarios, the exact shape and behavior of these curves would depend on the specific data and models used.
# Load the "iris" dataset
data(iris)
# EDA
summary(iris)
pairs(iris[, 1:4], main="Scatterplot Matrix", pch=21, bg=c("red", "green", "blue")[unclass(iris$Species)])
boxplot(iris[,1:4], main="Boxplots of Measurements")
# Setting a seed for reproducibility
set.seed(123)
# Splitting the dataset into 70% train and 30% test.
train_indices <- sample(1:nrow(iris), nrow(iris)*0.7)
train_set <- iris[train_indices, ]
test_set <- iris[-train_indices, ]
# Load the 'class' library
library(class)
# Predict species with k = 3
k3_pred <- knn(train=train_set[,1:4], test=test_set[,1:4], cl=train_set$Species, k=3)
# Predict species with k = 5
k5_pred <- knn(train=train_set[,1:4], test=test_set[,1:4], cl=train_set$Species, k=5)
# Predict species with k = 7
k7_pred <- knn(train=train_set[,1:4], test=test_set[,1:4], cl=train_set$Species, k=7)
# Using caret for cross-validation
library(ggplot2)
library(caret)
library(lattice)
# Define training control
train_control <- trainControl(method="cv", number=10)  # 10-fold CV
# Training model with train function (it will try different values of k)
model <- train(Species ~ ., data=train_set, method="knn", trControl=train_control, tuneGrid=data.frame(k=c(3,5,7)))
# Best k
best_k <- model$bestTune$k
# Load the 'class' library
library(class)
# Predict species with k = 3
k3_pred <- knn(train=train_set[,1:4], test=test_set[,1:4], cl=train_set$Species, k=3)
# Predict species with k = 5
k5_pred <- knn(train=train_set[,1:4], test=test_set[,1:4], cl=train_set$Species, k=5)
# Predict species with k = 7
k7_pred <- knn(train=train_set[,1:4], test=test_set[,1:4], cl=train_set$Species, k=7)
k3_pred
k5_pred
k7_pred
# Using caret for cross-validation
library(ggplot2)
library(caret)
library(lattice)
# Define training control
train_control <- trainControl(method="cv", number=10)  # 10-fold CV
# Training model with train function (it will try different values of k)
model <- train(Species ~ ., data=train_set, method="knn", trControl=train_control, tuneGrid=data.frame(k=c(3,5,7)))
# Best k
best_k <- model$bestTune$k
# Using the best k for prediction
final_pred <- knn(train=train_set[,1:4], test=test_set[,1:4], cl=train_set$Species, k=best_k)
# Misclassified observations
misclassified <- sum(final_pred != test_set$Species)
correctly_classified_percentage <- (nrow(test_set) - misclassified) / nrow(test_set) * 100
# Using the best k for prediction
final_pred <- knn(train=train_set[,1:4], test=test_set[,1:4], cl=train_set$Species, k=best_k)
# Misclassified observations
misclassified <- sum(final_pred != test_set$Species)
correctly_classified_percentage <- (nrow(test_set) - misclassified) / nrow(test_set) * 100
final_pred
misclassified
correctly_classified_percentage
